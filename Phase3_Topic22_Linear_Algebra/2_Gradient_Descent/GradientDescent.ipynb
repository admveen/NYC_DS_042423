{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-\\amily:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Gradient Descent\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC June 2023\n",
    "<p>Phase 3</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In data science:\n",
    "- Supervised learning\n",
    "- Trying to train model or input/output predictive function\n",
    "- Learn model parameters/weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Optimal weights/parameters:\n",
    "    \n",
    "- where defined cost function L is at a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With linear regression is squared loss:\n",
    "\n",
    "$$ L =  |\\textbf{y }- X \\textbf{w} |^2 $$\n",
    "\n",
    "for 1D linear regression is compact linear algebra notation for:\n",
    "\n",
    "$$ L =  \\sum_i|y_i - w_1 x_i - w_0 |^2 = \\\\ \\sum_i|y_i - \\hat{y}_i |^2  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Slope weight including bias. \n",
    "\n",
    "2D minimization problem.\n",
    "\n",
    "\n",
    "Parameters:\n",
    "- $w_0$, $w_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's find $w_0$ and $w_1$ that minimizes $L$:\n",
    "- Minimize partial derivatives: \n",
    "-  $\\frac{\\partial L}{\\partial w_0} = 0$ and $ \\frac{\\partial L}{\\partial w_1} = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"images\\local_minimum.jpg\" width = 300 />\n",
    "\n",
    "Minimum: partial derivates in **both** directions vanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Import sympy and do some mathemagic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "from sympy.abc import x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x, y, i, N, w0, w1 = symbols(\"x, y, i, N, w0, w1\")\n",
    "L = summation((Indexed('y',i) - Indexed('x',i)*w1 - w0)**2 ,(i,1,N))\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grad1 = diff(L, w0)\n",
    "grad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad2 = diff(L, w1)\n",
    "grad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Setting grad1 to 0, solve for $w_0$:\n",
    "$$ w_0 = \\bar{y} - w_1\\bar{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Plug in $w_0$, solve for $w_1$ setting grad2 to 0.\n",
    "\n",
    "Algebra happened:\n",
    "\n",
    "\n",
    "$$ w_1 = \\frac{ \\Big(\\sum_i^Nx_iy_i) - N \\bar{y}\\bar{x} }{\\Big(\\sum_i^Nx_i^2\\Big) - N \\bar{x}^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With ordinary least squares linear regression: there's always a formula.\n",
    "- Minimizing cost function\n",
    "- Analytically find solution for model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can also use a computer to iteratively converge to minimum and get weights.\n",
    "\n",
    "- Useful when you can't solve for weights analytically.\n",
    "- When cost function as a function of fit parameters is complex.\n",
    "- Most problems in machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution: Gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Look at it work for our simple linear regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/simple_regression.gif\" width = 500/></center>\n",
    "<center>What's it doing?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Randomly initialize weight vector $\\textbf{w} = \\textbf{w}^{(0)}$.\n",
    "2. Compute cost function $J(\\textbf{w}^{(0)})$.\n",
    "2. Compute gradient of cost $J$: Gradient is in direction of highest slope.\n",
    "3. Step weight vector in opposite direction for descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/grad_desc_2d.png\" width = 700/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. #### Engineering the weight vector step $\\Delta \\textbf{w}^{(0)}$\n",
    "\n",
    "- step weight vector in opposite direction of gradient for descent.\n",
    "- $\\Delta \\textbf{w}^{(0)}$ proportional to $ - \\nabla_\\textbf{w}J\\Big|_{\\textbf{w} = \\textbf{w}^{(0)}}$\n",
    "\n",
    "\n",
    "$$ \\textbf{w}^{(0)} \\rightarrow \\textbf{w}^{(0)} + \\Delta \\textbf{w}^{(0)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\textbf{w}^{(0)} \\rightarrow \\textbf{w}^{(0)} - \\alpha \\nabla_\\textbf{w}J\\Big|_{\\textbf{w} = \\textbf{w}^{(0)}} $$\n",
    "\n",
    "i.e., \n",
    "\n",
    "$$ \\Delta \\textbf{w}^{(0)} = - \\alpha \\nabla_\\textbf{w}J\\Big|_{\\textbf{w} = \\textbf{w}^{(0)}}$$\n",
    "\n",
    "where $\\alpha$ is a small parameter known as the **learning rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or component-wise:\n",
    "\n",
    "$$ w_1^{(0)} \\rightarrow w_1^{(0)} - \\alpha \\frac{\\partial J}{\\partial w_1}\\Big|_{\\textbf{w} = \\textbf{w}^{(0)}} $$\n",
    "\n",
    "\n",
    "$$ w_2^{(0)} \\rightarrow w_2^{(0)} - \\alpha \\frac{\\partial J}{\\partial w_2}\\Big|_{\\textbf{w} = \\textbf{w}^{(0)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/grad_desc_2d.png\" width = 700/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now at new weight vector $\\textbf{w}^{(1)} = \\begin{bmatrix}w_1^{(1)} & w_2^{(1)}\\end{bmatrix} $\n",
    "- Recompute cost function $J(w_1^{(1)}, w_2^{(1)})$\n",
    "- Recompute gradient vector  $\\nabla_\\textbf{w} J\\Big|_{\\textbf{w} = \\textbf{w}^{(1)}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Take step down in opposite gradient direction: $$ \\Delta \\textbf{w}^{(1)} = - \\alpha \\nabla_\\textbf{w} J\\Big|_{\\textbf{w} = \\textbf{w}^{(1)}} $$\n",
    "- Now we are at $\\textbf{w} = \\textbf{w}^{(2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pictorially, we are doing this at each step:\n",
    "\n",
    "<center><img src = \"Images/gradient_anim.gif\" width = 700 /> <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Play this game, until weights converge to minimum:\n",
    "- Closer to minimum: gradients gets small\n",
    "- weight updates get vanishingly small.\n",
    "- weights at/near optimal value.\n",
    "\n",
    "Kill the loop. We are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/simple_regression.gif\" width = 500 /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- But let's do this ourselves. Step by step.\n",
    "- First create some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "# Randomly created data in x & y\n",
    "np.random.seed(27)\n",
    "\n",
    "X = np.random.rand(30, 1)\n",
    "X = sm.add_constant(X) # add a constant\n",
    "y = np.random.normal(0, 3, 30).reshape(-1,1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Step 1: Initialize weights randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def init_weight(X):\n",
    "    num_features = X.shape[1]\n",
    "    w_0 = np.random.normal(loc =0 , \n",
    "                           scale = 4, \n",
    "                           size = (num_features,\n",
    "                                   1))\n",
    "    \n",
    "    return w_0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w_0 = init_weight(X)\n",
    "w_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Step 2: Compute cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In vector form with augmented $X$ is:\n",
    " $$ (\\textbf{y} - X \\textbf{w})^T(\\textbf{y} - X \\textbf{w}) = |\\textbf{y} - X \\textbf{w}|^2 $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# in vectorized form \n",
    "\n",
    "def comp_cost(X, y, w):\n",
    "    \n",
    "    costfunc = (y - X@w).T @ (y - X@w)\n",
    "    \n",
    "    return costfunc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "comp_cost(X, y, w_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Define and compute the gradient:\n",
    "    - $\\nabla_\\textbf{w} |\\textbf{y}- X\\textbf{w}|^2 = - 2 ( X^T\\textbf{y} - X^TX \\textbf{w} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X,y,w):\n",
    "    \n",
    "    grad = 2*((X.T@X)@w - X.T@y)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "q = compute_gradient(X,y,w_0)\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. Update your weights in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def update_weight(w, grad, learning_rate = 0.01):\n",
    "    dw = - learning_rate*grad \n",
    "    w_new = w + dw\n",
    "    \n",
    "    return w_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "update_weight(w_0, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now do all this in a loop and store cost function values and gradients along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent_optimizer(X, y, learning_rate = 0.01, num_iter = 250):\n",
    "    \n",
    "    #initialize\n",
    "    J = {}\n",
    "    grads = {}\n",
    "    w_history = {}\n",
    "    \n",
    "    w = init_weight(X)\n",
    "    \n",
    "    for trial in range(num_iter):\n",
    "        \n",
    "        # compute and store cost in dictionary\n",
    "        J[trial] = comp_cost(X, y ,w) \n",
    "        \n",
    "        # compute the gradient of J at current w\n",
    "        grads[trial] = compute_gradient(X, y, w)\n",
    "        \n",
    "        w = update_weight(w, grads[trial], learning_rate = learning_rate)\n",
    "        w_history[trial] = w\n",
    "        \n",
    "    # return final weight and cache\n",
    "    cache = {'cost': J, 'gradients': grads, 'weight_history': w_history}\n",
    "    return w, cache\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let punch the button and run the gradient descent on our least squares problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w, cache = gradient_descent_optimizer(X, y, learning_rate = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cache['cost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A plot of the cost function as a function of iteration is helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture cost_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('talk')\n",
    "\n",
    "trials = cache['cost'].keys()\n",
    "cost_history = cache['cost'].values()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot( x =trials, y = cost_history, marker = 's', linestyle = '--')\n",
    "ax.set_xlabel('Iteration number')\n",
    "ax.set_ylabel('Cost function')\n",
    "ax.set_title('Gradient descent: Cost Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cost_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compare result of gradient descent with analytical calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#gradient descent\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#formula\n",
    "import numpy as np\n",
    "w_analyt =np.linalg.solve(X.T@X, X.T@y)\n",
    "w0_true, w1_true = w_analyt\n",
    "w_analyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept = False)\n",
    "lr.fit(X,y)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's look at how the weights get iteratively optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "w, cache = gradient_descent_optimizer(X, y, learning_rate = 0.001, num_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w_0list = []\n",
    "w_1list = []\n",
    "for iteration, weight in cache['weight_history'].items():\n",
    "    w_0list.append(weight[0][0])\n",
    "    w_1list.append(weight[1][0])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x = w_0list, y = w_1list, ax = ax, label = 'Gradient descent')\n",
    "ax.scatter(w0_true, w1_true, s = 150, c ='r', label = 'Analytical')\n",
    "ax.set_ylabel('$w_1$')\n",
    "ax.set_xlabel('$w_0$')\n",
    "ax.legend()\n",
    "ax.set_title('History of weight optimization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Effect of the learning rate:\n",
    "    \n",
    "- Affects step size\n",
    "\n",
    "$$ \\Delta \\textbf{w} = - \\alpha \\nabla_\\textbf{w} J$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If our steps are _too big_, we risk skipping over the minimum value (optimal parameters).\n",
    "\n",
    "If our steps are _too small_, it might take us too long to reach the minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![learning_rate](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optimization can be made better: \n",
    "\n",
    "- many ways to improve the way we take steps\n",
    "- how we use our training data via sampling can help a lot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Most importantly\n",
    "\n",
    "- Gradient descent:\n",
    "    - will be using under the hood for most algorithms from now on.\n",
    "    - but in many cases want to tune how algorithm optimizes\n",
    "    - sometimes need to pop the hood and mess around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent in Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Make a guess at where the function attains its minimum value\n",
    "- Calculate the gradient/derivative at that point\n",
    "- Use that value to decide how to make your next guess!\n",
    "\n",
    "Repeat until we get the derivative as close as we like to 0.\n",
    "\n",
    "If we want to improve our guess at the minimum of our loss function, we'll move in the **opposite direction** of the gradient away from our last guess. Hence we are using the *gradient* of our loss function to *descend* to the minimum value of the relevant loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dplearn",
   "language": "python",
   "name": "dplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
